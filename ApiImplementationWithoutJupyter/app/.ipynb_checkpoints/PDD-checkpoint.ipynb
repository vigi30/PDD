{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67df6987-c184-41d3-abcf-3e3c9421a0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import shutil\n",
    "from fastapi import FastAPI,File, UploadFile\n",
    "import uvicorn\n",
    "from PIL import Image\n",
    "import nest_asyncio\n",
    "import dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#intializing the fast api object\n",
    "app = FastAPI()\n",
    "\n",
    "def run(app=app):\n",
    "    nest_asyncio.apply()\n",
    "    uvicorn.run(app, port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71fd7b4",
   "metadata": {},
   "source": [
    "#### Below code is the utility functions which will be used as a part of video detection.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# given a dataframe of all the detected object coordinates in a frame, put them in a list of tuples \n",
    "def get_coordinates(df):\n",
    "    objects = []\n",
    "    for index, row in df.iterrows():\n",
    "        objects.append( ( int(row[0]), int(row[1]), int(row[2]), int(row[3]) ) )\n",
    "\n",
    "    return objects\n",
    "\n",
    "# Takes frame and the model that got initialized. Then, process that frame and detects the desired objects and returns the coorindates, confidence and class label.\n",
    "# However, we are only taking the coordinates of the detected objects and returning that as a list of tuples.\n",
    "def detect(model, img):\n",
    "    results = model(img)\n",
    "    df = results.pandas().xyxy[0]\n",
    "    detect_info = get_coordinates(df)\n",
    "    return detect_info\n",
    "\n",
    "\n",
    "# Given the coordinates of the detected objects, return the centroids of those objects as a list of centroids.\n",
    "def get_centroid(detect_info):\n",
    "    # object --> (top-left) x1,y1, (bottom-right) x2,y2\n",
    "    centroids = []\n",
    "    for object in detect_info:\n",
    "        centroid_x = (object[0] + object[2]) // 2\n",
    "        centroid_y = (object[1] + object[3]) // 2\n",
    "        bboxHeight = object[3] - object[1]\n",
    "        centroids.append([centroid_x, centroid_y, bboxHeight])\n",
    "    return centroids\n",
    "\n",
    "\n",
    "# returns the distance between two centroids \n",
    "def get_distance(centroid1, centroid2):\n",
    "    distance = math.sqrt((centroid1[0] - centroid2[0]) ** 2 + (centroid1[1] - centroid2[1]) ** 2)\n",
    "    return distance\n",
    "\n",
    "# Given the centroids of the detected objects, computes  the distance between each centroids and those centroids which have less than MIN_DIST threshold will be marked as a violated people.\n",
    "def get_violated_distance_people_test(centroids, MIN_DIST=80):\n",
    "    violated_people = []\n",
    "    for i in range(len(centroids)):\n",
    "        for j in range(i + 1, len(centroids)):\n",
    "            if get_distance(centroids[i], centroids[j]) <= MIN_DIST:\n",
    "                violated_people.append((i, j))\n",
    "    return violated_people\n",
    "\n",
    "\n",
    "# Draws the rectangle around the detected objects with red color for violated people along with arrows and  \n",
    "def draw_arrows(image, detect_info, centroids, violated_people):\n",
    "    if len(detect_info) > 0:\n",
    "        for idx, object in enumerate(detect_info):\n",
    "            #checks if the current person or index is in violated people list, if yes, then draws a red color rectangale with label as ALert alongs with arrows\n",
    "            if list(filter(lambda x: x.count(idx) > 0, violated_people)):\n",
    "                temp = list(filter(lambda x: x.count(idx) > 0, violated_people))  # [(),()] temp - violated people index\n",
    "                color = (0, 0, 255)\n",
    "                label = 'Alert'\n",
    "                for i in range(len(temp)):\n",
    "                    # image = cv2.circle(image, (centroids[temp[i][0]][0], centroids[temp[i][0]][1]), 10, color, -1)\n",
    "                    # image = cv2.circle(image, (centroids[temp[i][1]][0], centroids[temp[i][1]][1]), 10, color, -1)\n",
    "                    image = cv2.arrowedLine(image, (centroids[temp[i][0]][0], centroids[temp[i][0]][1]),\n",
    "                                     (centroids[temp[i][1]][0], centroids[temp[i][1]][1]), color, 2)\n",
    "                    image = cv2.arrowedLine(image, (centroids[temp[i][1]][0], centroids[temp[i][1]][1]),(centroids[temp[i][0]][0], centroids[temp[i][0]][1]), color, 2)\n",
    "            else:\n",
    "                color = (0, 255, 0)\n",
    "                label = 'Safe'\n",
    "            image = cv2.rectangle(image, (object[0], object[1]), (object[2], object[3]), color, 2)\n",
    "            image = cv2.putText(image, label, (object[0], object[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2,cv2.LINE_AA)\n",
    "\n",
    "    text = \"Social Distancing Violations: {}\".format(len(set([item  for tup in violated_people for item in tup])))\n",
    "    image = cv2.putText(image, text, (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.85, (0, 0, 255), 3)\n",
    "    return image\n",
    "\n",
    "\n",
    "# This part of code will download the model weights and skeleton of the Yolov5{s/m/l}.\n",
    "# cls = 0 represents the person class which will make sure only the person object is detecteda and others are discarded.\n",
    "# conf_thresh  = Minimum probability of the detected objects\n",
    "# iou_thresh - Minimum threshold for IOU\n",
    "def get_model(algo = 'yolov5s',cls =0, conf_thresh=0.30,iou_thresh=0.30):\n",
    "    dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    # model = torch.hub.load('ultralytics/yolov5', 'yolov5s').to(dev) # force_reload=True\n",
    "    model = torch.hub.load('ultralytics/yolov5', algo,force_reload=True).to(dev)\n",
    "    model.classes = cls #person\n",
    "    model.conf = conf_thresh\n",
    "    model.iou = iou_thresh\n",
    "    # filename\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecc702d",
   "metadata": {},
   "source": [
    "#### The main part of this project-- Phydical distance detector is implemented below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "### The endpoint should return JSON-formatted information about the input video (physical distancing detections)\n",
    "\n",
    "# Takes the uploaded file (filename) and computes PDD and saves the processed video with saving_file_name under the folder output_files\n",
    "async def videodetect(model,filename, saving_file_name, isTracking =True):\n",
    "\n",
    "    '''\n",
    "    In order to reduce the time and memory consumption as well as flickering of the deteted object, I used detection once every 50 frames (skip_frames) \n",
    "     and in between used object tracking algorithm from dlib library.  \n",
    "\n",
    "    To use only detection algorithm, then, assign isTracking to False\n",
    "    \n",
    "    '''\n",
    "\n",
    "\n",
    "    filepath = os.path.join(filename)\n",
    "    cap = cv2.VideoCapture(filepath)\n",
    "    if cap.isOpened() ==False:\n",
    "      print(\"Error in opening up the file\")\n",
    "\n",
    "    if not os.path.exists('output_files'):\n",
    "        os.mkdir('output_files')\n",
    "    #stores all the coordinates, centroids and violated people of all the frames in the video    \n",
    "    detection_objects,all_centroids, all_violates = [],[],[]\n",
    "    fourcc = cv2.VideoWriter_fourcc('X','V','I','D')\n",
    "    save_file_path = os.path.join('output_files', saving_file_name)\n",
    "    writer = cv2.VideoWriter(save_file_path, fourcc, 25,( int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),   int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
    "\n",
    "    start = time.time()\n",
    "    num_frames = 0\n",
    "    skip_frames =50\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            # Need to convert the image to RGB format as dlib accepts only RGB format\n",
    "            RGBframe = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            # For every N frames we will use detection algorithm\n",
    "            if num_frames % skip_frames ==0:\n",
    "                detect_info = detect(model, RGBframe)\n",
    "                trackers = []\n",
    "                # Each detected object needs a tracker object which helps in tracking that object for the following frames \n",
    "                for object in detect_info:\n",
    "                    tracker = dlib.correlation_tracker()\n",
    "                    rect = dlib.rectangle(object[0],object[1],object[2],object[3])\n",
    "                    tracker.start_track(RGBframe, rect)\n",
    "                    trackers.append(tracker)\n",
    "                    # frame = cv2.rectangle(frame, (object[0],object[1]), (object[2],object[3]),(0, 255, 0), 2)\n",
    "                \n",
    "                # computing the distance between the objects and identifying the violated people in a frame\n",
    "                centroids = get_centroid(detect_info)\n",
    "                violated_people = get_violated_distance_people_test(centroids)\n",
    "                frame = draw_arrows(frame, detect_info, centroids, violated_people)\n",
    "\n",
    "                detection_objects.append(detect_info)\n",
    "                all_centroids.append(centroids)\n",
    "                all_violates.append(violated_people)\n",
    "            else:\n",
    "\n",
    "              # this part of code only uses object tracking, where each tracker (detected object) postion is updated based KCF tracking algorithm\n",
    "              detect_info = []\n",
    "              for tracker in trackers:\n",
    "\n",
    "                tracker.update(RGBframe)\n",
    "                pos = tracker.get_position()\n",
    "                startX = int(pos.left())\n",
    "                startY = int(pos.top())\n",
    "                endX = int(pos.right())\n",
    "                endY = int(pos.bottom())\n",
    "                detect_info.append((startX, startY, endX, endY))\n",
    "                # frame = cv2.rectangle(frame, (startX, startY), (endX, endY),(0, 255, 0), 2)\n",
    "\n",
    "              # computing the distance between the objects and identifying the violated people in a frame\n",
    "              centroids = get_centroid(detect_info)\n",
    "              violated_people = get_violated_distance_people_test(centroids)\n",
    "              frame = draw_arrows(frame, detect_info, centroids, violated_people)\n",
    "\n",
    "              detection_objects.append(detect_info)\n",
    "              all_centroids.append(centroids)\n",
    "              all_violates.append(violated_people)\n",
    "\n",
    "            # opencv-python/ opencv-python-contrib is not working in a container for the below api call (cv2.imshow) and hence live processing is not possible when running in container            \n",
    "            try:\n",
    "              cv2.imshow('Frame', frame)\n",
    "              if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "                break\n",
    "            except:\n",
    "              pass\n",
    "            writer.write(frame)\n",
    "            num_frames += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    end = time.time()\n",
    "    seconds = end - start\n",
    "    print(\"Time taken : {0} seconds\".format(seconds))\n",
    "    # Calculate frames per second\n",
    "    fps = num_frames / seconds\n",
    "    print(\"Estimated frames per second : {0}\".format(fps))\n",
    "    print(f\"Total number of frames processed: {num_frames}\")\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "    try:\n",
    "        cv2.destroyAllWindows()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    return (detection_objects,all_centroids,all_violates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0decc917",
   "metadata": {},
   "source": [
    "I am storing the uploaded file into the local storage under uploaded_files folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "async def _save_file_to_disk(uploaded_file, path=\".\", save_as=\"default\"):\n",
    "    # extension = os.path.splitext(uploaded_file.filename)[-1]\n",
    "    temp_file = os.path.join(path, save_as)\n",
    "    with open(temp_file, \"wb\") as buffer:\n",
    "        shutil.copyfileobj(uploaded_file.file, buffer)\n",
    "    return temp_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7df6a4",
   "metadata": {},
   "source": [
    "### The endpoint should accept a video via the body of a POST request (video can be small to circumvent large video issues)\n",
    " \n",
    "When the file is uploaded, the 'create_upload_file' function will be called first as this is our endpoint. This function will load the model, saves the uploaded file, and calls the videodetect function compute the PDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# You can change this to yolov5s,yolov5m,yolov5l which signifies different model configuration \n",
    "algo = 'yolov5s'\n",
    "\n",
    "@app.post(\"/uploadfile/\")\n",
    "async def create_upload_file(file: UploadFile= File(...)):\n",
    "    model = get_model(algo)\n",
    "    folder_name = 'uploaded_files'\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.mkdir(folder_name)\n",
    "    temp_file = await _save_file_to_disk(file, path=folder_name, save_as=file.filename)\n",
    "    detect_info,centroids,violated_people = await videodetect(model,temp_file,file.filename)\n",
    "    return {'detection_objects':detect_info, 'centroids':centroids, 'violated_people_pair' : violated_people}\n",
    "    # return {\"filename\": file.filename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [12480]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59413 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59415 - \"GET /openapi.json HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/archive/master.zip\" to C:\\Users\\deepi/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2022-3-26 torch 1.11.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken : 37.07526397705078 seconds\n",
      "Estimated frames per second : 2.939965581026474\n",
      "Total number of frames processed: 109\n",
      "INFO:     127.0.0.1:59420 - \"POST /uploadfile/ HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/archive/master.zip\" to C:\\Users\\deepi/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2022-3-26 torch 1.11.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken : 239.10304307937622 seconds\n",
      "Estimated frames per second : 2.9527018598656047\n",
      "Total number of frames processed: 706\n",
      "INFO:     127.0.0.1:59430 - \"POST /uploadfile/ HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "Application shutdown complete.\n",
      "INFO:     Finished server process [12480]\n",
      "Finished server process [12480]\n"
     ]
    }
   ],
   "source": [
    "# this will start the process and opens ups the API endpoint in the localhost.  Use http://localhost:8000/docs, this will provide web interface given by FastAPI,\n",
    "#  where you can  try to upload the file under the POST endpoint  'uploadfile'.\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Additional Questions\n",
    "\n",
    "<i><b>  a)  What are some of the challenges you faced while designing your PDD? Where might it fail? Please justify and make suggestions for improvement. </i></b>\n",
    "\n",
    "Ans:\n",
    "\n",
    "<i><u> Challenges 1) </i></u> I found that due to the pixel wise distance calculation, vertical distance between people in the image is lower compared to the ground truth. However, the horizontal distance between people is seems to be closer to the ground truth distance. Hence, people who are far apart from each other also violate the MINIMUM DISTANCE when measured vertically.\n",
    "\n",
    "\n",
    "<i><u> Solution 1) </i></u> : To use intrinsic and extrinsic parameters of camera caliberation, However it will be expensive to install and buy cameras.\n",
    "\n",
    "<i><u> Challenges 2) </i></u> If the model size is big, the processing of the image will be time-consuming.\n",
    "\n",
    "<i><u> Solution 2) </i></u>:  I have used object tracking algorithm along with object detection, where detection is executed once in every 10 or 20 frames and till the object tracking is executed which drastically improves the speed. Furthermore, using quantization and pruning, we can reduce the size of the detection model, but it will hurt the accuracy.\n",
    "\n",
    "\n",
    "<i><u> Bug 3) </i></u> Initializing opencv-python/opencv-contrib{headless} in docker with GUI does not work.\n",
    "\n",
    "<i><u> Solution 3) </i></u> Therefore, when installed in your local machine the live processing of the video will pop up. However, if you are running it through docker container, the video will processed in the background and stores in a folder.\n",
    "\n",
    "\n",
    " - https://github.com/opencv/opencv-python/issues/447#issuecomment-779847130\n",
    " - https://github.com/opencv/opencv-python/issues/370#issuecomment-671202529\n",
    "\n",
    "<i><b>  b) How could you improve your PDD so that it does not raise an alert when two people are from the same household (and therefore allowed to be close together)?</b></i>\n",
    "\n",
    "Ans : Possible solution is to check the minimum distance between two people for N number of frames, if it is less than some threshold than probably they are from same household or they know each other.\n",
    "\n",
    "Step 1) Is to keep track of each objects' centroid using centroid tracking algorithm, through which we can assign a unique ID to each object and keep track of it for N frames.{object_ID : centroids}\n",
    "\n",
    "Step 2) Fetch the ID's of people who violated the minimum distance rule and then check the mean of the distance between each of them for N frames, if their mean is less than some threshold then let's assume that they are from same household or they know each other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Optional\n",
    "\n",
    "<i><b>  c) How could you improve this so that the predicted bounding boxes do not flicker between frames? </i></b>\n",
    "\n",
    "Sol: I used object tracking along with object detection in order to get rid flickering. This resulted in lesser number of flickering than only using detection.\n",
    "\n",
    "<i><b> d)Write functions to implement a, b and/or c. </i></b>\n",
    "\n",
    " I solved the a) (challenge 2) and c) using the  <b>videodetect</b> function. By combining both detection and tracking methodology as explained in the previous cell. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
